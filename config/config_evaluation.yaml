main:
    #TODO: add the possibility to evaluate a model with a specific checkpoint
    #TODO: make this for run_evaluation.py and its pipeline
    # Necessary User-defined variables
    evaluate: True  # "Whether to create 1000 recipes in a file for evaluation purpose with ingredients from test set or simply generate recipes to print in console."
    model_type: 'gpt2'  # ['gpt2', 'opt-125m', 'opt-350m', 'qlora'] "Model type selected in the list"
    sample_file_path: ''  # "If we want to specify a relative path of a file containing the ingredients to generate recipes from."

    # Optional user-defined variables
    prompt: 'ham, egg, butter, milk, leek, sour creme, bread, sausage'  # "The prompt to test a single recipe generation."
    num_promps: 10  # "Number of recipes to generate."
    length: 700 # "The maximum number of tokens for each recipe to be generated. -1 is the max the model can handle."
    num_samples: 1  # "Number of samples to generate."
    temperature: 1.0  # "Close to 0 is more greedy, higher is more random"
    top_k: 0  # "The number of highest probability vocabulary tokens to keep for top-k-filtering."
    top_p: 0.9  # "The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling."

    # Hardware and logging pre-defined variables
    train_data_file: 'data/unsupervised_train_filtered.txt'  # "The input training data file (a text file)."
    eval_data_file: 'data/unsupervised_test_filtered.txt'   # "Data file unseen by the model (only to calculate perplexity of each checkpoint while training) to do evaluations (a text file)."
    output_dir: ''  # "The output directory where the model predictions and checkpoints will be written."
    output_dir_to_eval: None  # "The output directory where the model predictions checkpoints to eval are."
    no_cuda: False  # "Avoid using CUDA when available"
    n_gpu: 1  # Overwritten in the code (depends on the nb of GPUs)
    seed: 1234
    logging_steps: 50  # "Log every X updates steps."
    save_steps: 50000  # "Save checkpoint every X updates steps. (default=50, which is way too much data)"
    eval_all_checkpoints: False  # "Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number"
    overwrite_output_dir: True  # "Overwrite the content of the output directory"
    overwrite_cache: False  # "Overwrite the cached training and evaluation sets"
    aws_bucket: ''  # "Whether to upload to specified bucket."

    # Overwritten variables (depends on the model type)
    tokenizer_name: '' # "Pretrained tokenizer name or path"
    model_name_or_path: ''  # "The model checkpoint for weights initialization., Use '/' before the first directory."
    base_model: ''  # "The base model to use for the model initialization. Used for LoRa."
    do_lower_case: False  # "Set this flag if you are using an uncased model."
    truncation_side: 'right'  # "Truncation side selection (left or right)."

# Hydra
hydra:
    job:
        chdir: True  # To make hydra work like you used to know

# Default parameters for GPT2
gpt2:
    tokenizer_name: '/checkpoints/gpt2/'
    model_name_or_path: '/checkpoints/gpt2/'
    do_lower_case: False
    truncation_side: 'right'

# Default parameters for OPT-125m
opt-125m:
    tokenizer_name: '/facebook/opt-125m/'
    model_name_or_path: '/checkpoints/opt-125m/'
    do_lower_case: False
    truncation_side: 'right'

# Default parameters for OPT-350mm
opt-350m:
    tokenizer_name: '/facebook/opt-350m/'
    model_name_or_path: '/checkpoints/opt-350m/'
    do_lower_case: False
    truncation_side: 'right'

# Default parameters for Llama-qlora
qlora:
    tokenizer_name: 'huggyllama/llama-7b'  # Can also possibly use 'decapoda-research/llama-7b-hf'
    model_name_or_path: '/checkpoints/qlora/'      # Can also possibly use 'decapoda-research/llama-7b-hf'
    base_model: 'huggyllama/llama-7b'
    do_lower_case: False
    truncation_side: 'right'
