main:
    model_type: opt  # "Model type selected in the list"
    train_data_file: data/unsupervised_train.txt  # "The input training data file (a text file)."
    output_dir: outputs/  # "The output directory where the model predictions and checkpoints will be written."
    eval_data_file: data/unsupervised_test.txt   # "An optional input evaluation data file to evaluate the perplexity on (a text file)."
    output_dir_to_eval: None  # "The output directory where the model predictions checkpoints to eval are."

gpt2:
    model_type: gpt2  # "Model type selected in the list"
    tokenizer: gpt2
    model_name_or_path: '/checkpoints/gpt2/checkpoint-gpt2/'  # "The model checkpoint for weights initialization., Use '/' before the first directory."
    prompt: 'ham, egg, butter, milk, leek, sour creme, bread, sausage'
    num_promps: 5  # "Number of prompts to generate."
    num_samples: 1  # "Number of samples to generate."
    length: 600  # "Number of tokens to generate."
    temperature: 3.0 # "Close to 0 is more greedy, higher is more random"
    top_k: 3  # "The number of highest probability vocabulary tokens to keep for top-k-filtering."
    top_p: 0.9  # "The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling."
    no_cuda: False  # "Avoid using CUDA when available"
    n_gpu: 1  # Overwritten in the code (depends on the nb of GPUs)

    seed: 1234
    logging_steps: 50  # "Log every X updates steps."
    save_steps: 50000  # "Save checkpoint every X updates steps. (default=50, which is way too much data)"
    eval_all_checkpoints: False  # "Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number"
    overwrite_output_dir: True  # "Overwrite the content of the output directory"
    overwrite_cache: False  # "Overwrite the cached training and evaluation sets"
    aws_bucket: ''  # "Whether to upload to specified bucket."

opt:
    model_type: opt  # "Model type selected in the list"
    tokenizer: 'facebook/opt-125m'
    model_name_or_path: '/checkpoints/opt/checkpoint-opt/'  # "The model checkpoint for weights initialization., Use '/' before the first directory."
    prompt: 'ham, egg, butter, milk, leek, sour creme, bread, sausage'
    num_promps: 5  # "Number of prompts to generate."
    num_samples: 1  # "Number of samples to generate."
    length: 800  # "Number of tokens to generate."
    temperature: 1.0  # "Close to 0 is more greedy, higher is more random"
    top_k: 0  # "The number of highest probability vocabulary tokens to keep for top-k-filtering."
    top_p: 0.9  # "The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling."
    no_cuda: False  # "Avoid using CUDA when available"
    n_gpu: 1  # Overwritten in the code (depends on the nb of GPUs)

    seed: 1234
    logging_steps: 50  # "Log every X updates steps."
    save_steps: 50000  # "Save checkpoint every X updates steps. (default=50, which is way too much data)"
    eval_all_checkpoints: False  # "Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number"
    overwrite_output_dir: True  # "Overwrite the content of the output directory"
    overwrite_cache: False  # "Overwrite the cached training and evaluation sets"
    aws_bucket: ''  # "Whether to upload to specified bucket."

llama:
    model_type: opt  # "Model type selected in the list"
    og_model: 'facebook/opt-125m'
    model_name_or_path: '/models/checkpoint-gpt2/'  # "The model checkpoint for weights initialization., Use '/' before the first directory."
    prompt: 'ham, egg, butter, milk, leek, sour creme, bread, sausage'
    num_promps: 5  # "Number of prompts to generate."
    num_samples: 1  # "Number of samples to generate."
    length: 600  # "Number of tokens to generate."
    temperature: 1.0  # "Close to 0 is more greedy, higher is more random"
    top_k: 0  # "The number of highest probability vocabulary tokens to keep for top-k-filtering."
    top_p: 0.9  # "The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling."
    no_cuda: False  # "Avoid using CUDA when available"
    n_gpu: 1  # Overwritten in the code (depends on the nb of GPUs)
