main:

    # User-defined variables
    model_type: gpt2  # [gpt2, opt] "Model type selected in the list"
    prompt: 'ham, egg, butter, milk, leek, sour creme, bread, sausage'
#    prompt: 'mayonnaise, Dijon mustard, chicken, lemon, rice, vegetable oil, white sugar, cayenne pepper, Worcestershire sauce'
    num_promps: 10  # "Number of recipes to generate."
    length: -1 # "The maximum number of tokens for each recipe to be generated. -1 is the max the model can handle."

    # Hardware and logging pre-defined variables
    train_data_file: data/unsupervised_train_filtered.txt  # "The input training data file (a text file)."
    output_dir: outputs/  # "The output directory where the model predictions and checkpoints will be written."
    eval_data_file: data/unsupervised_test_filtered.txt   # "An optional input evaluation data file to evaluate the perplexity on (a text file)."
    output_dir_to_eval: None  # "The output directory where the model predictions checkpoints to eval are."
    no_cuda: False  # "Avoid using CUDA when available"
    n_gpu: 1  # Overwritten in the code (depends on the nb of GPUs)
    seed: 1234
    logging_steps: 50  # "Log every X updates steps."
    save_steps: 50000  # "Save checkpoint every X updates steps. (default=50, which is way too much data)"
    eval_all_checkpoints: False  # "Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number"
    overwrite_output_dir: True  # "Overwrite the content of the output directory"
    overwrite_cache: False  # "Overwrite the cached training and evaluation sets"
    aws_bucket: ''  # "Whether to upload to specified bucket."

    # Overwritten variables (depends on the model type)
    tokenizer: ''
    model_name_or_path: ''  # "The model checkpoint for weights initialization., Use '/' before the first directory."
    num_samples: 0  # "Number of samples to generate."
    temperature: 0 # "Close to 0 is more greedy, higher is more random"
    top_k: 0  # "The number of highest probability vocabulary tokens to keep for top-k-filtering."
    top_p: 0  # "The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling."

# Default parameters for GPT2
gpt2:
    tokenizer: gpt2
    model_name_or_path: '/checkpoints/gpt2/checkpoint-gpt2/'
    num_samples: 1
    temperature: 1.0
    top_k: 0
    top_p: 0.9

# Default parameters for OPT
opt:
    tokenizer: 'facebook/opt-125m'
    model_name_or_path: '/checkpoints/opt/checkpoint-opt-final/'
    num_samples: 1
    temperature: 1.0
    top_k: 0
    top_p: 0.9

# Default parameters for Llama
llama:
    tokenizer: 'decapoda-research/llama-7b-hf'
    model_name_or_path: '/checkpoints/llama/checkpoint-llama-final/'
    num_samples: 1
    temperature: 1.0
    top_k: 0
    top_p: 0.9
