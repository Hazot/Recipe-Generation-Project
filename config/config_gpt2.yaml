data:
    create_txt_files: False
    create_h5_file: False
    train_data_file: data/unsupervised_train.txt  # "The input training data file (a text file)."
    output_dir: outputs/  # "The output directory where the model predictions and checkpoints will be written."
    eval_data_file: None  # "An optional input evaluation data file to evaluate the perplexity on (a text file)."
    output_dir_to_eval: None  # "The output directory where the model predictions checkpoints to eval are."

alg:
    no_cuda: False  # "Avoid using CUDA when available"
    n_gpu: 1  # Overwritten in the code (depends on the nb of GPUs)
    model_name_or_path: gpt2  # "The model checkpoint for weights initialization."
    config_name: ''  # "Optional pretrained config name or path if not the same as model_name_or_path"
    cache_dir: ''  # "Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)"
    block_size: -1  # "Optional input sequence length after tokenization."
    tokenizer_name: gpt2  # "Optional pretrained tokenizer name or path if not the same as model_name_or_path"
    do_train: True  # "Whether to run training."
    do_eval: True  # "Whether to run eval on the dev set."
    evaluate_during_training: False  # "Run evaluation during training at each logging step."
    do_lower_case: False  # "Set this flag if you are using an uncased model."
    per_gpu_train_batch_size: 2  # "Batch size per GPU/CPU for training. (from paper, gpt-2 was 4)"
    per_gpu_eval_batch_size: 2  # "Batch size per GPU/CPU for evaluation. (from paper, gpt-2 was 4)"
    train_batch_size: -1  # Overwritten in the code (depends on the nb of GPUs)
    eval_batch_size: -1  # Overwritten in the code (depends on the nb of GPUs)
    gradient_accumulation_steps: 1  # "Number of updates steps to accumulate before performing a backward/update pass."
    learning_rate: 5e-5  # "The initial learning rate for Adam."
    weight_decay: 0.0  # "Weight decay if we apply some."
    adam_epsilon: 1e-8  # "Epsilon for Adam optimizer."
    max_grad_norm: 1.0  # "Max gradient norm."
    num_train_epochs: 1.0  # "Total number of training epochs to perform."
    max_steps: -1  # "If > 0: set total number of training steps to perform. Override num_train_epochs."
    warmup_steps: 0  # "Linear warmup over warmup_steps."

log:
    logging_steps: 50  # "Log every X updates steps."
    save_steps: 50000  # "Save checkpoint every X updates steps. (default=50, which is way too much data)"
    eval_all_checkpoints: False  # "Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number"
    overwrite_output_dir: True  # "Overwrite the content of the output directory"
    overwrite_cache: False  # "Overwrite the cached training and evaluation sets"
    aws_bucket: ''  # "Whether to upload to specified bucket."