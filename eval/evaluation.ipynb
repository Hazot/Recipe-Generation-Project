{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import transformers\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import GPT2Tokenizer, AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ingredients_list(recipe):\n",
    "    '''\n",
    "    Gets the list of input ingredients from a raw recipe\n",
    "    '''\n",
    "    ingr_start_index = recipe.find(\"<INPUT_START>\")\n",
    "    ingr_end_index = recipe.find(\"<INPUT_END>\")\n",
    "\n",
    "    ingredients_sequence = \" \".join(recipe[ingr_start_index + len(\"<INPUT_START>\"):ingr_end_index].strip().split())  # Find the input ingredients list sequence\n",
    "    ingredients_list = ingredients_sequence.split(\"<NEXT_INPUT>\")  # split the ingredients when the next input token is reached\n",
    "    return [x.strip() for x in ingredients_list]  # strip whitespaces before and after ingredients\n",
    "\n",
    "\n",
    "def print_raw_recipe(full_raw_recipe):\n",
    "    '''\n",
    "    Print a raw recipe (containing the special tokens) to be easier to read\n",
    "    '''\n",
    "    markdown = re.sub(\"<RECIPE_(START|END)>\", \"\", full_raw_recipe)\n",
    "    recipe_n_title = markdown.split(\"<TITLE_START>\")\n",
    "    title = \"# \" + recipe_n_title[1].replace(\"<TITLE_END>\", \"\") + \" #\\n\"\n",
    "    markdown = recipe_n_title[0].replace(\"<INPUT_START>\", \"## Input ingredients ##\\n`\").replace(\"<INPUT_END>\", \"`\\n\")\n",
    "    markdown = markdown.replace(\"<NEXT_INPUT>\", \"`\\n`\").replace(\"<INGR_START>\",\"## Ingredients ##\\n* \").replace(\"<NEXT_INGR>\",\"\\n* \").replace(\"<INGR_END>\", \"\\n\")\n",
    "    markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n1) \")\n",
    "\n",
    "    # Count each instruction\n",
    "    count = 2\n",
    "    while markdown.find(\"<NEXT_INSTR>\") != -1:\n",
    "        markdown = markdown.replace(\"<NEXT_INSTR>\", f\"\\n{count}) \", 1)\n",
    "        count += 1\n",
    "\n",
    "    markdown = markdown.replace(\"<INSTR_END>\", \"\\n\")\n",
    "    markdown = re.sub(\"$ +#\", \"#\", markdown)\n",
    "    markdown = re.sub(\"( +`|` +)\", \"`\", markdown)\n",
    "    print('\\n' + title + markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = os.path.normpath(os.getcwd() + os.sep + os.pardir)\n",
    "sample_path = local_path + \"/results/2023-07-20_12-25-38/sample_gpt2.txt\"\n",
    "finetuned_path = local_path + \"/results/2023-07-20_12-25-38/finetuned_gpt2.txt\"\n",
    "print(sample_path)\n",
    "print(finetuned_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir=\"data\"\n",
    "\n",
    "data = {\n",
    "    \"sample\": [],\n",
    "    \"finetuned\": [],\n",
    "    \"vanilla\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sample_path, 'r') as f:\n",
    "    content = f.readlines()\n",
    "    data[\"sample\"] = [content[i * 2].replace('\\n','') for i in range(len(content) // 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(finetuned_path, 'r') as f:\n",
    "    content = f.readlines()\n",
    "    data[\"finetuned\"] = [content[i * 2].replace('\\n','') for i in range(len(content) // 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data['sample']))\n",
    "print(len(data['finetuned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_raw_recipe(data['finetuned'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(local_path + '/checkpoints/gpt2/checkpoint-gpt2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tensor = [tokenizer.encode(recipe) for recipe in data['sample']]\n",
    "finetuned_tensor = [tokenizer.encode(recipe) for recipe in data['finetuned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = 0\n",
    "for k, rec1 in enumerate(sample_tensor):\n",
    "    best = 0\n",
    "    for i in range(0,10):\n",
    "        rec2 = finetuned_tensor[k*10 + i]\n",
    "        \n",
    "        # pad\n",
    "        pad_len = np.abs(len(rec1) - len(rec2))\n",
    "        if len(rec1) < len(rec2):\n",
    "            rec1.extend([0]*pad_len)\n",
    "        else:\n",
    "            rec2.extend([0]*pad_len)\n",
    "        \n",
    "        cos = cosine_similarity([rec1], [rec2])\n",
    "        best = max(best, cos)\n",
    "    avg += best\n",
    "\n",
    "avg = avg/len(sample_tensor)\n",
    "print(\"avg: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Language check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "#tool.disable_spellchecking()\n",
    "results = tool.check(data[\"finetuned\"][0])\n",
    "results_filtered = [result for result in results if result.ruleId!='WHITESPACE_RULE' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "#tool.disable_spellchecking()\n",
    "\n",
    "avg = 0\n",
    "dataset = \"finetuned\"\n",
    "\n",
    "for rec in tqdm(data[dataset], desc=\"Iteration\", disable=False, position=0, leave=True):\n",
    "    results = tool.check(rec)\n",
    "    results_filtered = [result for result in results if result.ruleId!='WHITESPACE_RULE' ]\n",
    "    avg += len(results_filtered)\n",
    "\n",
    "print(avg / len(data[dataset]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Readibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* textstat.smog_index(test_data)\n",
    "* textstat.flesch_kincaid_grade(test_data)\n",
    "* textstat.coleman_liau_index(test_data)\n",
    "* textstat.automated_readability_index(test_data)\n",
    "* textstat.dale_chall_readability_score(test_data)\n",
    "* textstat.difficult_words(test_data)\n",
    "* textstat.linsear_write_formula(test_data)\n",
    "* textstat.gunning_fog(test_data)\n",
    "* textstat.text_standard(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import textstat\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "dataset = \"finetuned\"\n",
    "\n",
    "for rec in data[dataset]:\n",
    "    result = textstat.flesch_reading_ease(rec)\n",
    "    #print(result)\n",
    "    ret.append(result)\n",
    "    \n",
    "print(np.mean(ret), np.median(ret), stats.mode(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "dataset = \"sample\"\n",
    "\n",
    "for rec in data[dataset]:\n",
    "    result = textstat.smog_index(rec)\n",
    "    #print(result)\n",
    "    ret.append(result)\n",
    "    \n",
    "print(np.mean(ret), np.median(ret), stats.mode(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "dataset = \"finetuned\"\n",
    "\n",
    "for rec in data[dataset]:\n",
    "    result = textstat.gunning_fog(rec)\n",
    "    #print(result)\n",
    "    ret.append(result)\n",
    "    \n",
    "print(np.mean(ret), np.median(ret), stats.mode(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "dataset = \"sample\"\n",
    "\n",
    "for rec in data[dataset]:\n",
    "    result = textstat.dale_chall_readability_score(rec)\n",
    "    #print(result)\n",
    "    ret.append(result)\n",
    "    \n",
    "print(np.mean(ret), np.median(ret), stats.mode(ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.translate.bleu_score as bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import nltk.translate.gleu_score as gleu\n",
    "import nltk.translate.meteor_score as meteor\n",
    "from jiwer import wer, mer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer_count(hyp, ref, print_matrix=False):\n",
    "    N = len(hyp)\n",
    "    M = len(ref)\n",
    "    L = np.zeros((N,M))\n",
    "    for i in range(0, N):\n",
    "        for j in range(0, M):\n",
    "            if min(i,j) == 0:\n",
    "                L[i,j] = max(i,j)\n",
    "            else:\n",
    "                deletion = L[i-1,j] + 1\n",
    "                insertion = L[i,j-1] + 1\n",
    "                sub = 1 if hyp[i] != ref[j] else 0\n",
    "                substitution = L[i-1,j-1] + sub\n",
    "                L[i,j] = min(deletion, min(insertion, substitution))\n",
    "    return int(L[N-1, M-1])\n",
    "\n",
    "def bleu_score(recipe, refer):\n",
    "    hyp = recipe\n",
    "    refs = refer\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    score_ref_a = bleu.sentence_bleu(refs, hyp, smoothing_function=smoothie)\n",
    "    return score_ref_a\n",
    "\n",
    "def gleu_score(recipe, refer):\n",
    "    hyp = recipe\n",
    "    refs = refer\n",
    "    score_ref_a = gleu.sentence_gleu(refs, hyp)\n",
    "    return score_ref_a\n",
    "\n",
    "def wer_score(recipe, refer):\n",
    "    hyp = recipe\n",
    "    refs = refer\n",
    "\n",
    "    mn = 99999\n",
    "    for ref in refs:\n",
    "        b = wer(ref, hyp)\n",
    "        mn = min(mn, b)\n",
    "       \n",
    "    return mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "for k, rec1 in enumerate(tqdm(data[\"sample\"])):\n",
    "    rec2 = data[\"finetuned\"][k*10: k*10 + 10]\n",
    "    res = bleu_score(rec1, rec2)\n",
    "    ret.append(res)\n",
    "\n",
    "np.mean(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "for k, rec1 in enumerate(tqdm(data[\"sample\"])):\n",
    "    rec2 = data[\"finetuned\"][k*10: k*10 + 10]\n",
    "    res = gleu_score(rec1, rec2)\n",
    "    ret.append(res)\n",
    "\n",
    "np.mean(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "for k, rec1 in enumerate(tqdm(data[\"sample\"])):\n",
    "    rec2 = data[\"finetuned\"][k*10: k*10 + 10]\n",
    "    res = wer_score(rec1, rec2)\n",
    "    ret.append(res)\n",
    "\n",
    "np.mean(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingredients evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if all the ingredients from the input list are used inside the generated instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instructions(recipe):\n",
    "    '''\n",
    "    Gets the string sequence of instructions from a raw recipe\n",
    "    '''\n",
    "    instr_start_index = recipe.find(\"<INSTR_START>\")\n",
    "    instr_end_index = recipe.find(\"<INSTR_END>\")\n",
    "\n",
    "    instruction_sequence = \" \".join(recipe[instr_start_index + len(\"<INSTR_START>\"):instr_end_index].strip().split())  # Find the input ingredients list sequence\n",
    "    instructions = instruction_sequence.split(\"<NEXT_INSTR>\")  # split the ingredients when the next input token is reached\n",
    "    instructions = [x.strip() for x in instructions]  # strip whitespaces before and after ingredients\n",
    "    return \" \".join(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ingredient_coverage_in_instructions(recipes):\n",
    "    results = []\n",
    "    for recipe in recipes:\n",
    "        ingredients = get_ingredients_list(recipe)\n",
    "        instructions = get_instructions(recipe).lower()\n",
    "\n",
    "        ingredients = list(dict.fromkeys(ingredients))  # remove duplicate ingredient to remove bias\n",
    "\n",
    "        count = sum([1 if re.search(ingredient.lower(), instructions) else 0 for ingredient in ingredients])\n",
    "    \n",
    "        results.append(count / len(ingredients))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_ingredient_coverage_in_instructions(data['finetuned'])\n",
    "print(np.mean(res))\n",
    "print(np.argmin(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = data['finetuned'][-5]\n",
    "recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ingredients_list(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_raw_recipe(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = get_ingredients_list(recipe)\n",
    "ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = get_instructions(recipe)\n",
    "instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_cleared = list(dict.fromkeys(ingredients))  # remove duplicate ingredient to remove bias\n",
    "for ingr in ingredients_cleared:\n",
    "    x = re.search(ingr, instructions)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if all the ingredients from the ingredients list are used inside the generated instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if there are any duplicates ingredients in the input list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if there are any duplicates ingredients in the ingredient list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if all the ingredients found in the generated instructions are mentioned in the input list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if all the ingredients found in the generated instructions are mentioned in the ingredients list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlg (py3.9-cu11.7)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
