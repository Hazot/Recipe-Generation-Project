{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hazot/miniconda3/envs/nlg/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import transformers\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import GPT2Tokenizer, AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_raw_recipe(full_raw_recipe):\n",
    "    '''\n",
    "    Print a raw recipe (containing the special tokens) to be easier to read\n",
    "    '''\n",
    "    markdown = re.sub(\"<RECIPE_(START|END)>\", \"\", full_raw_recipe)\n",
    "    recipe_n_title = markdown.split(\"<TITLE_START>\")\n",
    "    title = \"# \" + recipe_n_title[1].replace(\"<TITLE_END>\", \"\") + \" #\\n\"\n",
    "    markdown = recipe_n_title[0].replace(\"<INPUT_START>\", \"## Input ingredients ##\\n`\").replace(\"<INPUT_END>\", \"`\\n\")\n",
    "    markdown = markdown.replace(\"<NEXT_INPUT>\", \"`\\n`\").replace(\"<INGR_START>\",\"## Ingredients ##\\n* \").replace(\"<NEXT_INGR>\",\"\\n* \").replace(\"<INGR_END>\", \"\\n\")\n",
    "    markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n1) \")\n",
    "\n",
    "    # Count each instruction\n",
    "    count = 2\n",
    "    while markdown.find(\"<NEXT_INSTR>\") != -1:\n",
    "        markdown = markdown.replace(\"<NEXT_INSTR>\", f\"\\n{count}) \", 1)\n",
    "        count += 1\n",
    "\n",
    "    markdown = markdown.replace(\"<INSTR_END>\", \"\\n\")\n",
    "    markdown = re.sub(\"$ +#\", \"#\", markdown)\n",
    "    markdown = re.sub(\"( +`|` +)\", \"`\", markdown)\n",
    "    print('\\n' + title + markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hazot/code/recipe-generation-project/results/2023-07-25_16-14-49/sample_gpt2.txt\n",
      "/home/hazot/code/recipe-generation-project/results/2023-07-25_16-14-49/finetuned_gpt2.txt\n"
     ]
    }
   ],
   "source": [
    "local_path = os.path.normpath(os.getcwd() + os.sep + os.pardir)\n",
    "sample_path = local_path + \"/results/2023-07-25_16-14-49/sample_gpt2.txt\"\n",
    "finetuned_path = local_path + \"/results/2023-07-25_16-14-49/finetuned_gpt2.txt\"\n",
    "print(sample_path)\n",
    "print(finetuned_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir=\"data\"\n",
    "\n",
    "data = {\n",
    "    \"sample\": [],\n",
    "    \"finetuned\": [],\n",
    "    \"vanilla\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sample_path, 'r') as f:\n",
    "    content = f.readlines()\n",
    "    data[\"sample\"] = [content[i * 2].replace('\\n','') for i in range(len(content) // 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(finetuned_path, 'r') as f:\n",
    "    content = f.readlines()\n",
    "    data[\"finetuned\"] = [content[i * 2].replace('\\n','') for i in range(len(content) // 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(data['sample']))\n",
    "print(len(data['finetuned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#  Spiced Christmas Gingersnaps   #\n",
      " ## Input ingredients ##\n",
      "`Mix`\n",
      "`baking soda`\n",
      "`cream of tartar`\n",
      "`egg`\n",
      "`cinnamon`\n",
      "`ginger`\n",
      "`cloves`\n",
      "`nutmeg`\n",
      "`salt`\n",
      "`white sugar`\n",
      "`butter`\n",
      "`sugar`\n",
      "`cinnamon`\n",
      " ## Ingredients ##\n",
      "*  2-1/2 qt. (26 cups) Nestle Quik Original Baking Mix \n",
      "*  1 tsp. baking soda \n",
      "*  1 tsp. cream of tartar \n",
      "*  1 egg, beaten \n",
      "*  1 tsp. cinnamon \n",
      "*  1 tsp. ginger \n",
      "*  1 tsp. cloves \n",
      "*  1/4 tsp. nutmeg \n",
      "*  1/2 tsp. salt \n",
      "*  1 cup white sugar \n",
      "*  2 Tbsp. butter or margarine \n",
      "*  1 cup sugar \n",
      "*  1 tsp. cinnamon \n",
      " ## Instructions ##\n",
      "1)  Combine dry mix, baking soda, cream of tartar, egg, spices and salt; mix well. \n",
      "2)  Gradually add sugar, stirring until well blended. \n",
      "3)  Shape dough into 1-in. balls. \n",
      "4)  Place on ungreased cookie sheets. \n",
      "5)  Press into 1/2-in. circles with bottom of glass dipped in sugar. \n",
      "6)  Bake at 325° for 8-10 minutes or until lightly browned. \n",
      "7)  Cool 1 minute on cookie sheets; remove to wire racks. \n",
      "8)  Sift 2 Tbsp. of the sugar and cinnamon over each cookie. \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print_raw_recipe(data['finetuned'][21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(local_path + '/checkpoints/gpt2/checkpoint-gpt2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tensor = [tokenizer.encode(recipe) for recipe in data['sample']]\n",
    "finetuned_tensor = [tokenizer.encode(recipe) for recipe in data['finetuned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = 0\n",
    "for k, rec1 in enumerate(sample_tensor):\n",
    "    best = 0\n",
    "    for i in range(0,10):\n",
    "        rec2 = finetuned_tensor[k*10 + i]\n",
    "        \n",
    "        # pad\n",
    "        pad_len = np.abs(len(rec1) - len(rec2))\n",
    "        if len(rec1) < len(rec2):\n",
    "            rec1.extend([0]*pad_len)\n",
    "        else:\n",
    "            rec2.extend([0]*pad_len)\n",
    "        \n",
    "        cos = cosine_similarity([rec1], [rec2])\n",
    "        best = max(best, cos)\n",
    "    avg += best\n",
    "\n",
    "avg = avg/len(sample_tensor)\n",
    "print(\"avg: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Language check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "#tool.disable_spellchecking()\n",
    "results = tool.check(data[\"finetuned\"][0])\n",
    "results_filtered = [result for result in results if result.ruleId!='WHITESPACE_RULE' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "#tool.disable_spellchecking()\n",
    "\n",
    "avg = 0\n",
    "dataset = \"finetuned\"\n",
    "\n",
    "for rec in tqdm(data[dataset], desc=\"Iteration\", disable=False, position=0, leave=True):\n",
    "    results = tool.check(rec)\n",
    "    results_filtered = [result for result in results if result.ruleId!='WHITESPACE_RULE' ]\n",
    "    avg += len(results_filtered)\n",
    "\n",
    "print(avg / len(data[dataset]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Readibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* textstat.smog_index(test_data)\n",
    "* textstat.flesch_kincaid_grade(test_data)\n",
    "* textstat.coleman_liau_index(test_data)\n",
    "* textstat.automated_readability_index(test_data)\n",
    "* textstat.dale_chall_readability_score(test_data)\n",
    "* textstat.difficult_words(test_data)\n",
    "* textstat.linsear_write_formula(test_data)\n",
    "* textstat.gunning_fog(test_data)\n",
    "* textstat.text_standard(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import textstat\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "dataset = \"finetuned\"\n",
    "\n",
    "for rec in data[dataset]:\n",
    "    result = textstat.flesch_reading_ease(rec)\n",
    "    #print(result)\n",
    "    ret.append(result)\n",
    "    \n",
    "print(np.mean(ret), np.median(ret), stats.mode(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "dataset = \"sample\"\n",
    "\n",
    "for rec in data[dataset]:\n",
    "    result = textstat.smog_index(rec)\n",
    "    #print(result)\n",
    "    ret.append(result)\n",
    "    \n",
    "print(np.mean(ret), np.median(ret), stats.mode(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "dataset = \"finetuned\"\n",
    "\n",
    "for rec in data[dataset]:\n",
    "    result = textstat.gunning_fog(rec)\n",
    "    #print(result)\n",
    "    ret.append(result)\n",
    "    \n",
    "print(np.mean(ret), np.median(ret), stats.mode(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "dataset = \"sample\"\n",
    "\n",
    "for rec in data[dataset]:\n",
    "    result = textstat.dale_chall_readability_score(rec)\n",
    "    #print(result)\n",
    "    ret.append(result)\n",
    "    \n",
    "print(np.mean(ret), np.median(ret), stats.mode(ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.translate.bleu_score as bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import nltk.translate.gleu_score as gleu\n",
    "import nltk.translate.meteor_score as meteor\n",
    "from jiwer import wer, mer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer_count(hyp, ref, print_matrix=False):\n",
    "    N = len(hyp)\n",
    "    M = len(ref)\n",
    "    L = np.zeros((N,M))\n",
    "    for i in range(0, N):\n",
    "        for j in range(0, M):\n",
    "            if min(i,j) == 0:\n",
    "                L[i,j] = max(i,j)\n",
    "            else:\n",
    "                deletion = L[i-1,j] + 1\n",
    "                insertion = L[i,j-1] + 1\n",
    "                sub = 1 if hyp[i] != ref[j] else 0\n",
    "                substitution = L[i-1,j-1] + sub\n",
    "                L[i,j] = min(deletion, min(insertion, substitution))\n",
    "    return int(L[N-1, M-1])\n",
    "\n",
    "def bleu_score(recipe, refer):\n",
    "    hyp = recipe\n",
    "    refs = refer\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    score_ref_a = bleu.sentence_bleu(refs, hyp, smoothing_function=smoothie)\n",
    "    return score_ref_a\n",
    "\n",
    "def gleu_score(recipe, refer):\n",
    "    hyp = recipe\n",
    "    refs = refer\n",
    "    score_ref_a = gleu.sentence_gleu(refs, hyp)\n",
    "    return score_ref_a\n",
    "\n",
    "def wer_score(recipe, refer):\n",
    "    hyp = recipe\n",
    "    refs = refer\n",
    "\n",
    "    mn = 99999\n",
    "    for ref in refs:\n",
    "        b = wer(ref, hyp)\n",
    "        mn = min(mn, b)\n",
    "       \n",
    "    return mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "for k, rec1 in enumerate(tqdm(data[\"sample\"])):\n",
    "    rec2 = data[\"finetuned\"][k*10: k*10 + 10]\n",
    "    res = bleu_score(rec1, rec2)\n",
    "    ret.append(res)\n",
    "\n",
    "np.mean(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "for k, rec1 in enumerate(tqdm(data[\"sample\"])):\n",
    "    rec2 = data[\"finetuned\"][k*10: k*10 + 10]\n",
    "    res = gleu_score(rec1, rec2)\n",
    "    ret.append(res)\n",
    "\n",
    "np.mean(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "for k, rec1 in enumerate(tqdm(data[\"sample\"])):\n",
    "    rec2 = data[\"finetuned\"][k*10: k*10 + 10]\n",
    "    res = wer_score(rec1, rec2)\n",
    "    ret.append(res)\n",
    "\n",
    "np.mean(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingredients evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if all the ingredients from the input list are used inside the generated instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_ingredients_list(recipe):\n",
    "    '''\n",
    "    Gets the list of input ingredients from a raw recipe\n",
    "    '''\n",
    "    ingr_start_index = recipe.find(\"<INPUT_START>\")\n",
    "    ingr_end_index = recipe.find(\"<INPUT_END>\")\n",
    "\n",
    "    ingredients_sequence = \" \".join(recipe[ingr_start_index + len(\"<INPUT_START>\"):ingr_end_index].strip().split())  # Find the input ingredients list sequence\n",
    "    ingredients_list = ingredients_sequence.split(\"<NEXT_INPUT>\")  # split the ingredients when the next input token is reached\n",
    "    return [x.strip() for x in ingredients_list]  # strip whitespaces before and after ingredients\n",
    "\n",
    "\n",
    "def get_instructions(recipe):\n",
    "    '''\n",
    "    Gets the string sequence of instructions from a raw recipe\n",
    "    '''\n",
    "    instr_start_index = recipe.find(\"<INSTR_START>\")\n",
    "    instr_end_index = recipe.find(\"<INSTR_END>\")\n",
    "\n",
    "    instruction_sequence = \" \".join(recipe[instr_start_index + len(\"<INSTR_START>\"):instr_end_index].strip().split())  # Find the input ingredients list sequence\n",
    "    instructions = instruction_sequence.split(\"<NEXT_INSTR>\")  # split the ingredients when the next input token is reached\n",
    "    instructions = [x.strip() for x in instructions]  # strip whitespaces before and after ingredients\n",
    "    return \" \".join(instructions)\n",
    "\n",
    "\n",
    "def input_ingredients_coverage_in_instructions(recipe):\n",
    "    '''\n",
    "    Returns the percentage of the number of ingredients from the input list that are actually present in the instructions for one recipe.\n",
    "    '''\n",
    "    ingredients = get_input_ingredients_list(recipe)\n",
    "    number_of_ingredients = len(ingredients)  # keeps the number of ingredients before removing duplicates\n",
    "    instructions = get_instructions(recipe).lower()\n",
    "\n",
    "    ingredients = list(dict.fromkeys(ingredients))  # remove duplicate ingredients to reduce bias\n",
    "    nb_ingr_found = sum([1 if ingredient.lower() in instructions else 0 for ingredient in ingredients])  # Gets the number of ingredients found in the instructions\n",
    "\n",
    "    return nb_ingr_found/number_of_ingredients\n",
    "\n",
    "\n",
    "def evaluate_recipes_input_ingredients_coverage_in_instructions(recipes):\n",
    "    '''\n",
    "    Evaluation on all the generated recipes (finetuned) for the coverage of the input list in the instructions.\n",
    "    Returns a list of percentage for the number of ingredients from the input list that are actually present in the instructions.\n",
    "    '''\n",
    "    results = []\n",
    "    for recipe in recipes:\n",
    "        results.append(input_ingredients_coverage_in_instructions(recipe))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average percentage of input ingredients covered: 0.5537150981253905\n",
      "Number of 0% coverage of input ingredients: 57\n"
     ]
    }
   ],
   "source": [
    "res = evaluate_recipes_input_ingredients_coverage_in_instructions(data['finetuned'])\n",
    "print('Average percentage of input ingredients covered:', np.mean(res))\n",
    "nb_of_zeros = sum([1 if x == 0.0 else 0 for x in res])\n",
    "print('Number of 0% coverage of input ingredients:', nb_of_zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONLY TESTING PURPOSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average percentage of input ingredients covered: 0.5261598679098679\n",
      "Number of 0% coverage of input ingredients: 11\n"
     ]
    }
   ],
   "source": [
    "# Checking on the samples to see how many zeros there are, therfore checking how many \"mix, combine, there are still in the training data\".\n",
    "res = evaluate_recipes_input_ingredients_coverage_in_instructions(data['sample'])\n",
    "print('Average percentage of input ingredients covered:', np.mean(res))\n",
    "nb_of_zeros = sum([1 if x == 0.0 else 0 for x in res])\n",
    "print('Number of 0% coverage of input ingredients:', nb_of_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "15\n",
      "16\n",
      "29\n",
      "30\n",
      "39\n",
      "49\n",
      "61\n",
      "69\n",
      "73\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(res):\n",
    "    if x == 0.0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#  Double Chocolate Oatmeal Cookies   #\n",
      " ## Input ingredients ##\n",
      "`sugar`\n",
      "`margarine`\n",
      "`egg`\n",
      "`water`\n",
      "`vanilla`\n",
      "`flour`\n",
      "`cocoa`\n",
      "`soda`\n",
      "`salt`\n",
      "`oats`\n",
      "`chocolate chips`\n",
      " ## Ingredients ##\n",
      "*  1 1/2 c. sugar \n",
      "*  1 c. margarine \n",
      "*  1 egg \n",
      "*  1/4 c. water \n",
      "*  1 tsp. vanilla \n",
      "*  1 1/4 c. flour \n",
      "*  1/3 c. cocoa \n",
      "*  1/2 tsp. soda \n",
      "*  1/2 tsp. salt \n",
      "*  3 c. oats (quick cooking) \n",
      "*  1 pkg. chocolate chips \n",
      " ## Instructions ##\n",
      "1)  Mix first 5 ingredients well. \n",
      "2)  Add the rest at once. \n",
      "3)  Put rounded teaspoonfuls 2 inches apart on ungreased pan. \n",
      "4)  Bake at 350° for 10 to 12 minutes. \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print_raw_recipe(data['sample'][15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if all the ingredients from the input list are in the ingredient list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listed_ingredients_list(recipe):\n",
    "    '''\n",
    "    Gets the string sequence of listed ingredients (list with quantities) from a raw recipe\n",
    "    '''\n",
    "    ingr_start_index = recipe.find(\"<INGR_START>\")\n",
    "    ingr_end_index = recipe.find(\"<INGR_END>\")\n",
    "\n",
    "    ingredients_sequence = \" \".join(recipe[ingr_start_index + len(\"<INGR_START>\"):ingr_end_index].strip().split())  # Find the input ingredients list sequence\n",
    "    ingredients_list = ingredients_sequence.split(\"<NEXT_INGR>\")  # split the ingredients when the next input token is reached\n",
    "    ingredients_list = [x.strip() for x in ingredients_list]  # strip whitespaces before and after ingredients\n",
    "    return \" \".join(ingredients_list)\n",
    "\n",
    "\n",
    "def input_ingredients_coverage_in_listed_ingredients(recipe):\n",
    "    '''\n",
    "    Returns the percentage of the number of ingredients from the input list that are actually present in the listed ingredients (list with quantities) for one recipe.\n",
    "    '''\n",
    "    input_ingredients = get_input_ingredients_list(recipe)  # Gets input ingredients (without quantities)\n",
    "    number_of_ingredients = len(input_ingredients)  # keeps the number of ingredients before removing duplicates\n",
    "    \n",
    "    listed_ingredients = get_listed_ingredients_list(recipe).lower()  # Gets listed ingredients (the one with quanities)\n",
    "\n",
    "    ingredients = list(dict.fromkeys(input_ingredients))  # remove duplicate ingredients to reduce bias\n",
    "    nb_ingredients_found = sum([1 if input_ingredient.lower() in listed_ingredients else 0 for input_ingredient in input_ingredients])  # Gets the number of ingredients found in the listed ingredients\n",
    "    return nb_ingredients_found / number_of_ingredients\n",
    "\n",
    "\n",
    "def evaluate_recipes_input_ingredients_coverage_in_listed_ingredients(recipes):\n",
    "    '''\n",
    "    Evaluation on all the generated recipes (finetuned) for the coverage of the input list in the listed ingredients (list with quantities).\n",
    "    Returns a list of percentage for the number of ingredients from the input list that are actually present in the listed ingredients.\n",
    "    '''\n",
    "    results = []\n",
    "    for recipe in recipes:\n",
    "        results.append(input_ingredients_coverage_in_listed_ingredients(recipe))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average percentage of input ingredients covered: 0.9887254493485531\n",
      "Number of non-100% coverage of input ingredients: 77\n"
     ]
    }
   ],
   "source": [
    "res = evaluate_recipes_input_ingredients_coverage_in_listed_ingredients(data['finetuned'])\n",
    "print('Average percentage of input ingredients covered:', np.mean(res))\n",
    "indx_where_not_one = [i for i, x in enumerate(res) if x != 1.0]\n",
    "print('Number of non-100% coverage of input ingredients:', len(indx_where_not_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9090909090909091"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ingredients_coverage_in_listed_ingredients(data['finetuned'][13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#  Apricot Spice Cookies Recipe   #\n",
      " ## Input ingredients ##\n",
      "`butter`\n",
      "`sugar`\n",
      "`eggs`\n",
      "`sour cream`\n",
      "`Apricot brandy`\n",
      "`Vanilla`\n",
      "`rum`\n",
      "`All-purpose`\n",
      "`Salt`\n",
      "`Baking soda`\n",
      "`Black pepper`\n",
      " ## Ingredients ##\n",
      "*  1/2 c. Unsalted butter \n",
      "*  1 c. Sugar \n",
      "*  3 Large eggs \n",
      "*  1 c. Lowfat sour cream \n",
      "*  1/2 c. Apricot brandy \n",
      "*  1 tsp Vanilla \n",
      "*  1/4 tsp Nutmeg (or possibly 1/4 c.) \n",
      "*  2 1/4 c. All-purpose flour \n",
      "*  1 tsp Salt \n",
      "*  1/2 tsp Baking soda \n",
      "*  1/2 tsp Black pepper \n",
      " ## Instructions ##\n",
      "1)  Heat oven to 350 degrees. \n",
      "2)  In large mixing bowl, cream butter and sugar. \n",
      "3)  Add in Large eggs, sour cream, Apricot brandy and vanilla. \n",
      "4)  In medium bowl, sift together flour, salt, baking soda and pepper. \n",
      "5)  Add in to egg mix alternately with flour mix. \n",
      "6)  Stir till just mixed. \n",
      "7)  Drop by teaspoonful on ungreased cookie sheet. \n",
      "8)  Bake at 350 degrees for 12 to 14 min. \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print_raw_recipe(data['finetuned'][13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if there are any duplicates ingredients in the input list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_duplicated_input_ingredients(recipes):\n",
    "    '''\n",
    "    Returns percentage of recipes without duplicated inputs\n",
    "    '''\n",
    "    count = 0\n",
    "    for recipe in recipes:\n",
    "        ingredients = get_input_ingredients_list(recipe)\n",
    "        filtered_list = list(dict.fromkeys(ingredients))\n",
    "        if len(ingredients) == len(filtered_list):\n",
    "            count +=1\n",
    "    return count / len(recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.757"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_duplicated_input_ingredients(data['finetuned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = ['test', 'test', 'x', 'a', 'b', 'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = list(dict.fromkeys(ingredients))\n",
    "l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if there are any duplicates ingredients in the ingredient list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if all the ingredients found in the generated instructions are mentioned in the input list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if all the ingredients found in the generated instructions are mentioned in the ingredients list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlg (py3.9-cu11.7)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
