{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c179c6f9-5f3f-4742-a73b-59d40da0cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import boto3\n",
    "import shutil\n",
    "import fire\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import hydra\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hydra.utils import get_original_cwd\n",
    "from omegaconf import DictConfig\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb3cf166-2d7d-4564-87df-4f8481e2c0e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "local_path = os.path.normpath(os.getcwd() + os.sep + os.pardir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c4fb8-b5f9-4f7c-955d-989d79731dc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GPT2 and OPT tokenizer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8574982c-e1f5-4d99-bc82-6f8163135232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e097839c-008a-45f8-9f5a-e51c9fe1fe20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer GPT2\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2',do_lower_case=False, truncation_side='left')\n",
    "\n",
    "special_tokens = {\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<TITLE_START>\",\n",
    "        \"<TITLE_END>\",\n",
    "        \"<INSTR_START>\",\n",
    "        \"<NEXT_INSTR>\",\n",
    "        \"<INSTR_END>\",\n",
    "        \"<INGR_START>\",\n",
    "        \"<NEXT_INGR>\",\n",
    "        \"<INGR_END>\",\n",
    "        \"<RECIPE_START>\",\n",
    "        \"<RECIPE_END>\",\n",
    "        \"<INPUT_START>\",\n",
    "        \"<INPUT_END>\",\n",
    "        \"<NEXT_INPUT>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "# tokenizer.pad_token_id = (\n",
    "#     0  # unk. we want this to be different from the eos token\n",
    "# )\n",
    "# tokenizer.padding_side = \"right\"  # Left: Allows batched inference, we put right for this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6843f25c-18a0-4747-941c-e33e3dfe04ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_gpt2 = local_path + \"/data/unsupervised.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806fe05f-59df-47de-935a-481f1bb4296b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5py.File(path_gpt2, 'r') as f:\n",
    "    data_np = f['train'][:]\n",
    "    train_dataset = torch.tensor(f['train'][:]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fa3a14-0d3a-43dc-967f-6aa2946e97c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a68479-69e0-4c70-a4ee-e2cc3ed8c573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "line = train_dataloader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8d669b6-d14e-47f9-bf3d-309adf7d3ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '!', 'additional_special_tokens': ['<TITLE_START>', '<TITLE_END>', '<INSTR_START>', '<NEXT_INSTR>', '<INSTR_END>', '<INGR_START>', '<NEXT_INGR>', '<INGR_END>', '<RECIPE_START>', '<RECIPE_END>', '<INPUT_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1df9f150-2a59-4138-83b3-b6bb513914bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<RECIPE_START> <INPUT_START> balsamic vinegar <NEXT_INPUT> sugar <NEXT_INPUT> water <NEXT_INPUT> watermelon <NEXT_INPUT> mint <INPUT_END> <INGR_START> 1/2 cup good balsamic vinegar <NEXT_INGR> 1/4 cup sugar <NEXT_INGR> 1/4 cup water <NEXT_INGR> 1/2 large watermelon <NEXT_INGR> 6 fresh mint leaves, julienned <INGR_END> <INSTR_START> Combine balsamic, sugar and water in saucepan. <NEXT_INSTR> Heat over medium-high heat and reduce slightly until syrup consistency, about 12 minutes. <NEXT_INSTR> Cool to room temperature. <NEXT_INSTR> Cut up watermelon in wedges. <NEXT_INSTR> Drizzle cooled balsamic syrup over watermelon. <NEXT_INSTR> Garnish with mint. <INSTR_END> <TITLE_START> Watermelon with Sweet Balsamic Syrup and Fresh Mint <TITLE_END> <RECIPE_END> <RECIPE_START> <INPUT_START> brown sugar <NEXT_INPUT> margarine <NEXT_INPUT> butter <NEXT_INPUT> crackers <NEXT_INPUT> chocolate chips <INPUT_END> <INGR_START> 3/4 c. brown sugar <NEXT_INGR> 1 stick margarine <NEXT_INGR> 1 stick butter <NEXT_INGR> 40 saltine crackers (more or less) <NEXT_INGR> 12 oz. chocolate chips <INGR_END> <INSTR_START> In a medium saucepan, combine the first 3 ingredients and boil for exactly 3 minutes. <NEXT_INSTR> Put heavy foil on cookie sheet. <NEXT_INSTR> Place 40 saltine crackers or how many it takes to cover pan. <NEXT_INSTR> Pour syrup over crackers. <NEXT_INSTR> Bake in a 400° oven for 5 minutes. <NEXT_INSTR> When removed from oven, sprinkle with the chocolate chips. <NEXT_INSTR> Let them melt and smooth over crackers. <NEXT_INSTR> Sprinkle with nuts. <NEXT_INSTR> Let cool to set. <INSTR_END> <TITLE_START> Toffee Bars <TITLE_END> <RECIPE_END> <RECIPE_START> <INPUT_START> stuffing <NEXT_INPUT> margarine <NEXT_INPUT> zucchini <NEXT_INPUT> onion <NEXT_INPUT> cream of chicken soup <NEXT_INPUT> sour cream <NEXT_INPUT> carrots <INPUT_END> <INGR_START> 1 (8 oz.) Pepperidge Farm stuffing <NEXT_INGR> 1 stick margarine <NEXT_INGR> 6 c. zucchini, diced <NEXT_INGR> 1/4 c. onion, chopped <NEXT_INGR> 1 can cream of chicken soup <NEXT_INGR> 1 c. sour cream <NEXT_INGR> 1 c. carrots, shredded <INGR_END> <INSTR_START> Saute zucchini and onion until color changes (around 5 minutes). <NEXT_INSTR> Add soup, sour cream and carrots; stir until blended. Melt margarine; add to stuffing mix. <NEXT_INSTR> Divide mixture in half. Pour 1/2 in casserole dish. <NEXT_INSTR> Add zucchini mixture. <NEXT_INSTR> Pour remaining stuffing mixture on top. <NEXT_INSTR> Bake for 45 minutes at 350°. <INSTR_END> <TITLE_START> Zucchini Casserole <TITLE_END> <RECIPE_END> <RECIPE_START> <INPUT_START> ground beef <NEXT_INPUT> tortilla chips <NEXT_INPUT> onion <NEXT_INPUT> Cheddar cheese <NEXT_INPUT> enchilada sauce <NEXT_INPUT> green chilies <NEXT_INPUT> cream of mushroom soup <NEXT_INPUT> cream of chicken soup <INPUT_END> <INGR_START> 2 lb. ground beef, browned and drained <NEXT_INGR> tortilla chips <NEXT_INGR> chopped onion <NEXT_INGR> 2 c. Cheddar cheese, grated <NEXT_INGR> 1 can enchilada sauce (hot or mild, to your taste) <NEXT_INGR> 1 can green chilies <NEXT_INGR> 1 can cream of mushroom soup <NEXT_INGR> 1 can cream of chicken soup <INGR_END> <INSTR_START> Mix the enchilada sauce, chilies and soups together. <NEXT_INSTR> Layer as follows in casserole dish: chips; half of beef; layer of onions, about 1/2 cup; 1 cup cheese; half of soup mixture. <NEXT_INSTR> Repeat layers, top with soup mixture. <NEXT_INSTR> Bake at 350° about 45 minutes. <INSTR_END> <TITLE_START> Enchilada Casserole <TITLE_END> <RECIPE_END> <RECIPE_START> <INPUT_START> beef stock <NEXT_INPUT> Cajun Seasoning <NEXT_INPUT> cooked meat <NEXT_INPUT> soy sauce <NEXT_INPUT> eggs <NEXT_INPUT> green onions <INPUT_END> <INGR_START> 1 (8-ounce) package spaghetti <NEXT_INGR> 2 quarts beef stock <NEXT_INGR> 1 teaspoon Cajun Seasoning <NEXT_INGR> 2 cups cooked meat (beef, chicken, pork, shrimp, turtle), finely chopped <NEXT_INGR> 2 tablespoons soy sauce <NEXT_INGR> 3 hard-boiled eggs, peeled and halved lengthwise <NEXT_INGR> 1 bunch green onions, finely chopped <INGR_END> <INSTR_START> Cook spaghetti according to package directions. While spaghetti is cooking, bring stock and Cajun Seasoning to a boil in a large pot. Add meat and simmer 5 minutes. Reserving stock, strain meat out. Set meat aside and keep warm. Bring stock back to a boil and stir in soy sauce. Simmer 1 minute. <NEXT_INSTR> To assemble, place drained spaghetti in 6 individual bowls. Divide meat and egg halves over spaghetti. Ladle on broth and sprinkle with green onions. <INSTR_END> <TITLE_START> Yaka Mein <TITLE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(line, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6923c42c-15d8-46e5-a945-b4d13e00f8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer OPT\n",
    "model_path = local_path + \"/checkpoints/opt/checkpoint-opt-final\"\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, do_lower_case=False, truncation_side='left')\n",
    "max_token_len = tokenizer.max_model_input_sizes[\"gpt2\"]\n",
    "\n",
    "special_tokens = {\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<TITLE_START>\",\n",
    "        \"<TITLE_END>\",\n",
    "        \"<INSTR_START>\",\n",
    "        \"<NEXT_INSTR>\",\n",
    "        \"<INSTR_END>\",\n",
    "        \"<INGR_START>\",\n",
    "        \"<NEXT_INGR>\",\n",
    "        \"<INGR_END>\",\n",
    "        \"<RECIPE_START>\",\n",
    "        \"<RECIPE_END>\",\n",
    "        \"<INPUT_START>\",\n",
    "        \"<INPUT_END>\",\n",
    "        \"<NEXT_INPUT>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1b4d30f-43a5-4dfc-83c1-3d07c7085091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_opt = local_path + \"/data/unsupervised_opt.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4472771f-f0e3-4666-88c2-0f99fded8b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5py.File(path_opt, 'r') as f:\n",
    "    data_np = f['train'][:]\n",
    "    train_dataset = torch.tensor(f['train'][:]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "671270d3-9e27-4dcc-9a0d-42b247e399d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8badb417-ab4c-4515-bbb2-991fda5d1599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "line = train_dataloader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a48c3d14-0beb-4a71-a90a-650a8edf38c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<RECIPE_START> <INPUT_START> sweet cherries <NEXT_INPUT> strawberries <NEXT_INPUT> white sugar <NEXT_INPUT> lemon juice <INPUT_END> <INGR_START> 1 1/2 cups sweet cherries <NEXT_INGR> 1 1/2 cups strawberries <NEXT_INGR> 1 cup white sugar <NEXT_INGR> 1 tablespoon lemon juice <INGR_END> <INSTR_START> Wash fruit. <NEXT_INSTR> Remove stems fro strawberries and cut up into large chunks. <NEXT_INSTR> Remove pits from cherries and cut cherries into halves. <NEXT_INSTR> Crush fruit- now you should have about 2 cups of fruit total ( once crushed.). <NEXT_INSTR> Place everything into the breadmaker - JAM CYCLE. <NEXT_INSTR> When cycle is finished, pour immediately into glass container (covered). and place into refridgerator. Note that the jam comes out runny but eith thicken up as it cools. <NEXT_INSTR> Store in refridgerator. <INSTR_END> <TITLE_START> Cherry Strawberry Preserves For The Bread Machine (Abm) <TITLE_END> <RECIPE_END> \\n <RECIPE_START> <INPUT_START> sweetbreads <NEXT_INPUT> milk <NEXT_INPUT> bechamel <NEXT_INPUT> eggs <NEXT_INPUT> butter <INPUT_END> <INGR_START> 1 each sweetbreads <NEXT_INGR> 1 cup milk <NEXT_INGR> 1 cup bechamel (white) sauce <NEXT_INGR> 3 large eggs <NEXT_INGR> 2 tablespoons butter <INGR_END> <INSTR_START> Soak the sweetbreads in cold water; drain and cover with the milk, bring to boiling point and simmer for 25 minutes. <NEXT_INSTR> Cool, remove membrane and cut in small neat cubes. <NEXT_INSTR> Serve with sauce. <INSTR_END> <TITLE_START> Creamed Sweetbreads <TITLE_END> <RECIPE_END> \\n <RECIPE_START> <INPUT_START> milk <NEXT_INPUT> sugar <NEXT_INPUT> banana <NEXT_INPUT> strawberry <INPUT_END> <INGR_START> 3 cups cold low-fat milk <NEXT_INGR> 1 sugar-free instant vanilla pudding mix (4 serving size) <NEXT_INGR> 1 medium banana, sliced <NEXT_INGR> 1 cup strawberry, halved <INGR_END> <INSTR_START> In blender, combine milk, pudding mix, bananas, and berries. <NEXT_INSTR> Blend on low for 1 minute, or until smooth. <NEXT_INSTR> Pour into glasses and serve immediately. <INSTR_END> <TITLE_START> Strawberry-Banana Swirl <TITLE_END> <RECIPE_END> \\n <RECIPE_START> <INPUT_START> tortilla chips <NEXT_INPUT> butter <NEXT_INPUT> cream cheese <NEXT_INPUT> eggs <NEXT_INPUT> green chilies <NEXT_INPUT> fresh jalapenos <NEXT_INPUT> colby cheese <NEXT_INPUT> cheese <NEXT_INPUT> sour cream <NEXT_INPUT> scallion <NEXT_INPUT> tomato <NEXT_INPUT> black olives <INPUT_END> <INGR_START> 1 cup crushed tortilla chips <NEXT_INGR> 3 tablespoons melted butter <NEXT_INGR> 16 ounces cream cheese, softened <NEXT_INGR> 2 eggs <NEXT_INGR> 1 (4 ounce) canchopped green chilies <NEXT_INGR> 2 fresh jalapenos, chopped <NEXT_INGR> 4 ounces colby cheese, shredded <NEXT_INGR> 4 ounces monterey jack cheese, shredded <NEXT_INGR> 14 cup sour cream <NEXT_INGR> chopped scallion <NEXT_INGR> chopped tomato <NEXT_INGR> sliced black olives <INGR_END> <INSTR_START> Mix the crushed tortilla chips and melted butter in a bowl. <NEXT_INSTR> Press over the bottom of a 9 inch springform pan. <NEXT_INSTR> Bake at 325 for 15 minutes. <NEXT_INSTR> Beat the cream cheese and eggs. <NEXT_INSTR> Mix in the green chilis, jalapenos, cheeses. <NEXT_INSTR> Pour over the baked layer. <NEXT_INSTR> Bake at 325 for 30 minutes. <NEXT_INSTR> Cool for 5 minutes. <NEXT_INSTR> Place on serving platter, loosen the sides of the pan with a knife and remove. <NEXT_INSTR> Spread the sour cream over the pie and garnish by mounding the chopped tomatoes in the middle, the olives on top in the middle and scattering the chopped scallions around the perimeter of the chopped tomatoes. <NEXT_INSTR> Arrange regular tortilla chips all around with points out and scatter a few red tortilla chips around. <INSTR_END> <TITLE_START> Chili Cheese Pie <TITLE_END> <RECIPE_END> \\n <RECIPE_START> <INPUT_START> head cabbage <NEXT_INPUT> green bell peppers <NEXT_INPUT> carrots <NEXT_INPUT> salt <NEXT_INPUT> sugar <NEXT_INPUT> vinegar <NEXT_INPUT> water <NEXT_INPUT> mustard <NEXT_INPUT> celery <INPUT_END> <INGR_START> 1 small or medium head cabbage (1 qt. chopped or shredded) <NEXT_INGR> 1/2 c. green bell peppers, chopped <NEXT_INGR> 1/2 c. grated carrots <NEXT_INGR> 1 Tbsp. salt <NEXT_INGR> 1 c. sugar <NEXT_INGR> 1/2 c. vinegar <NEXT_INGR> 1/2 c. water <NEXT_INGR> 1 tsp. prepared mustard <NEXT_INGR> 2 c. celery <INGR_END> <INSTR_START> From a small or medium head of cabbage, chop or shred enough to equal 1 quart. <NEXT_INSTR> Add the bell peppers, carrots and 1 tablespoon salt. <NEXT_INSTR> Soak this mixture for 1 hour, then drain. <NEXT_INSTR> Take the sugar, vinegar, 1/2 cup water and prepared mustard. <NEXT_INSTR> Let come to a boil. Let cool. <NEXT_INSTR> Add the celery. <NEXT_INSTR> Pour this mixture over the drained cabbage mix. <NEXT_INSTR> Put in jars; keep refrigerated. <INSTR_END> <TITLE_START> Cooked Slaw <TITLE_END> <RECIPE_END> \\n <RECIPE_START> <INPUT_START> margarine <NEXT_INPUT> sugar <NEXT_INPUT> eggs <NEXT_INPUT> white bread <NEXT_INPUT> pineapple <INPUT_END> <INGR_START> 1/2 c. margarine <NEXT_INGR> 1 c. sugar <NEXT_INGR> 4 eggs, beaten <NEXT_INGR> 5 slices white bread, cubed <NEXT_INGR> 1 large can crushed pineapple, drained <INGR_END> <INSTR_START> Cream margarine and sugar. <NEXT_INSTR> Mix in the other ingredients. Pour into buttered 8-inch square pan. <NEXT_INSTR> Bake at 350° for 1 hour. <INSTR_END> <TITLE_START> Pineapple Spoonbread <TITLE_END> <RECIPE_END> \\n</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(line, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a89a25-609e-48d6-a79e-c556fca7cd18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8fb836d-2867-465c-b6d2-91fb5f1fa3cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test Llama tokenizer and h5 dataset to use huggingface Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "064dee98-3604-4e4c-a510-40c3737e11c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "#tokenizer Llama\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    'decapoda-research/llama-7b-hf',\n",
    "    do_lower_case=False,\n",
    "    truncation_side='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aeb1657-c963-4e1a-87ed-7e7bff9af78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<TITLE_START>\",\n",
    "        \"<TITLE_END>\",\n",
    "        \"<INSTR_START>\",\n",
    "        \"<NEXT_INSTR>\",\n",
    "        \"<INSTR_END>\",\n",
    "        \"<INGR_START>\",\n",
    "        \"<NEXT_INGR>\",\n",
    "        \"<INGR_END>\",\n",
    "        \"<RECIPE_START>\",\n",
    "        \"<RECIPE_END>\",\n",
    "        \"<INPUT_START>\",\n",
    "        \"<INPUT_END>\",\n",
    "        \"<NEXT_INPUT>\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"right\"  # Left: Allows batched inference, we put right for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc89801-0642-4f09-bea4-0d068929217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = os.path.normpath(os.getcwd() + os.sep + os.pardir)\n",
    "path = local_path + \"/data/unsupervised_llama.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e271fac6-6360-46c8-b4cd-e1bd379a574c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['test', 'train']>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path, 'r') as f:\n",
    "    print(f.keys())\n",
    "    raw_data = f['train']\n",
    "    train_dataset = torch.tensor(f['train'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8115da2-1678-44ea-b4db-07c312a94034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([319961, 2048])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "babe1a0e-0836-4b8f-934c-e45ad9d31f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Closed HDF5 dataset>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3ba6ff3-6566-451b-9f13-3baadb604580",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a pyarrow.Table or a datasets.table.Table object, but got {'train': tensor([[32008, 32010,   289,  ...,     0,     0,     0],\n        [32008, 32010,  1205,  ...,     0,     0,     0],\n        [32008, 32010,   330,  ...,     0,     0,     0],\n        ...,\n        [32008, 32010, 18423,  ...,     0,     0,     0],\n        [32008, 32010,   796,  ...,     0,     0,     0],\n        [32008, 32010,   367,  ...,     0,     0,     0]])}.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m datta \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlg/lib/python3.9/site-packages/datasets/arrow_dataset.py:644\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, arrow_table, info, split, indices_table, fingerprint)\u001b[0m\n\u001b[1;32m    641\u001b[0m DatasetInfoMixin\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, info\u001b[38;5;241m=\u001b[39minfo, split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m    642\u001b[0m IndexableMixin\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data: Table \u001b[38;5;241m=\u001b[39m \u001b[43m_check_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrow_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices: Optional[Table] \u001b[38;5;241m=\u001b[39m _check_table(indices_table) \u001b[38;5;28;01mif\u001b[39;00m indices_table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    646\u001b[0m maybe_register_dataset_for_temp_dir_deletion(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlg/lib/python3.9/site-packages/datasets/arrow_dataset.py:607\u001b[0m, in \u001b[0;36m_check_table\u001b[0;34m(table)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m table\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a pyarrow.Table or a datasets.table.Table object, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected a pyarrow.Table or a datasets.table.Table object, but got {'train': tensor([[32008, 32010,   289,  ...,     0,     0,     0],\n        [32008, 32010,  1205,  ...,     0,     0,     0],\n        [32008, 32010,   330,  ...,     0,     0,     0],\n        ...,\n        [32008, 32010, 18423,  ...,     0,     0,     0],\n        [32008, 32010,   796,  ...,     0,     0,     0],\n        [32008, 32010,   367,  ...,     0,     0,     0]])}."
     ]
    }
   ],
   "source": [
    "datta = datasets.Dataset({'train':train_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55942e50-2c26-4278-a184-2f0b083c1acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = datasets.DatasetDict({'train':train_dataset}, features=['input', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6670b07c-8e82-410f-83f1-7a21ecf0a344",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'features'"
     ]
    }
   ],
   "source": [
    "dset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f15b1b2-b974-4f47-a488-e4decf999784",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dict = {\n",
    "    'train': [train_dataset]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725f0719-574e-4b81-8088-5e94fdfc1ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "px = pd.DataFrame(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e12d95-f6ce-4b4e-a7ed-863874e8f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d423e7e-3b9a-418d-848b-3edf062e4392",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not Dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlg/lib/python3.9/site-packages/datasets/load.py:1749\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1743\u001b[0m     verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mNO_CHECKS \u001b[38;5;28;01mif\u001b[39;00m ignore_verifications \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1744\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore_verifications\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m was deprecated in favor of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverification_mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in version 2.9.1 and will be removed in 3.0.0.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can remove this warning by passing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverification_mode=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mverification_mode\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1747\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m   1748\u001b[0m     )\n\u001b[0;32m-> 1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATASET_STATE_JSON_FILENAME\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1751\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to load a dataset that was saved using `save_to_disk`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1752\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `load_from_disk` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1753\u001b[0m     )\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming \u001b[38;5;129;01mand\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlg/lib/python3.9/pathlib.py:1082\u001b[0m, in \u001b[0;36mPath.__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m Path:\n\u001b[1;32m   1081\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m WindowsPath \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m PosixPath\n\u001b[0;32m-> 1082\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour\u001b[38;5;241m.\u001b[39mis_supported:\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1085\u001b[0m                               \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,))\n",
      "File \u001b[0;32m~/miniconda3/envs/nlg/lib/python3.9/pathlib.py:707\u001b[0m, in \u001b[0;36mPurePath._from_parts\u001b[0;34m(cls, args, init)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_parts\u001b[39m(\u001b[38;5;28mcls\u001b[39m, args, init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# We need to call _parse_args on the instance, so as to get the\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# right flavour.\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 707\u001b[0m     drv, root, parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drv \u001b[38;5;241m=\u001b[39m drv\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root \u001b[38;5;241m=\u001b[39m root\n",
      "File \u001b[0;32m~/miniconda3/envs/nlg/lib/python3.9/pathlib.py:691\u001b[0m, in \u001b[0;36mPurePath._parse_args\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    689\u001b[0m     parts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39m_parts\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 691\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;66;03m# Force-cast str subclasses to str (issue #21127)\u001b[39;00m\n\u001b[1;32m    694\u001b[0m         parts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(a))\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not Dataset"
     ]
    }
   ],
   "source": [
    "dset = load_dataset(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d25311-a678-454e-ac9b-20ea950c0138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc5eca9-8a29-4684-9256-dba1faaabdfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb5d52c-a409-4f5f-a369-dadd956e809a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31c5be76-24a7-40f2-95da-b536605da84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3d6f3f1-1f80-412b-a3b9-b89ace2f2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13fdc7c7-45d7-41f4-b91e-f1daddb1f5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32008, 32010,   289,  ...,     0,     0,     0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ecf677b-4e57-4f43-b181-f3151386aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = train_dataloader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d4f5641-cd0e-425b-ad23-acd7de3cefdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<RECIPE_START> <INPUT_START> balsamic vinegar <NEXT_INPUT> sugar <NEXT_INPUT> water <NEXT_INPUT> watermelon <NEXT_INPUT> mint <INPUT_END> <INGR_START> 1/2 cup good balsamic vinegar <NEXT_INGR> 1/4 cup sugar <NEXT_INGR> 1/4 cup water <NEXT_INGR> 1/2 large watermelon <NEXT_INGR> 6 fresh mint leaves, julienned <INGR_END> <INSTR_START> Combine balsamic, sugar and water in saucepan. <NEXT_INSTR> Heat over medium-high heat and reduce slightly until syrup consistency, about 12 minutes. <NEXT_INSTR> Cool to room temperature. <NEXT_INSTR> Cut up watermelon in wedges. <NEXT_INSTR> Drizzle cooled balsamic syrup over watermelon. <NEXT_INSTR> Garnish with mint. <INSTR_END> <TITLE_START> Watermelon with Sweet Balsamic Syrup and Fresh Mint <TITLE_END> <RECIPE_END> <RECIPE_START> <INPUT_START> brown sugar <NEXT_INPUT> margarine <NEXT_INPUT> butter <NEXT_INPUT> crackers <NEXT_INPUT> chocolate chips <INPUT_END> <INGR_START> 3/4 c. brown sugar <NEXT_INGR> 1 stick margarine <NEXT_INGR> 1 stick butter <NEXT_INGR> 40 saltine crackers (more or less) <NEXT_INGR> 12 oz. chocolate chips <INGR_END> <INSTR_START> In a medium saucepan, combine the first 3 ingredients and boil for exactly 3 minutes. <NEXT_INSTR> Put heavy foil on cookie sheet. <NEXT_INSTR> Place 40 saltine crackers or how many it takes to cover pan. <NEXT_INSTR> Pour syrup over crackers. <NEXT_INSTR> Bake in a 400° oven for 5 minutes. <NEXT_INSTR> When removed from oven, sprinkle with the chocolate chips. <NEXT_INSTR> Let them melt and smooth over crackers. <NEXT_INSTR> Sprinkle with nuts. <NEXT_INSTR> Let cool to set. <INSTR_END> <TITLE_START> Toffee Bars <TITLE_END> <RECIPE_END> <RECIPE_START> <INPUT_START> stuffing <NEXT_INPUT> margarine <NEXT_INPUT> zucchini <NEXT_INPUT> onion <NEXT_INPUT> cream of chicken soup <NEXT_INPUT> sour cream <NEXT_INPUT> carrots <INPUT_END> <INGR_START> 1 (8 oz.) Pepperidge Farm stuffing <NEXT_INGR> 1 stick margarine <NEXT_INGR> 6 c. zucchini, diced <NEXT_INGR> 1/4 c. onion, chopped <NEXT_INGR> 1 can cream of chicken soup <NEXT_INGR> 1 c. sour cream <NEXT_INGR> 1 c. carrots, shredded <INGR_END> <INSTR_START> Saute zucchini and onion until color changes (around 5 minutes). <NEXT_INSTR> Add soup, sour cream and carrots; stir until blended. Melt margarine; add to stuffing mix. <NEXT_INSTR> Divide mixture in half. Pour 1/2 in casserole dish. <NEXT_INSTR> Add zucchini mixture. <NEXT_INSTR> Pour remaining stuffing mixture on top. <NEXT_INSTR> Bake for 45 minutes at 350°. <INSTR_END> <TITLE_START> Zucchini Casserole <TITLE_END> <RECIPE_END> <RECIPE_START> <INPUT_START> ground beef <NEXT_INPUT> tortilla chips <NEXT_INPUT> onion <NEXT_INPUT> Cheddar cheese <NEXT_INPUT> enchilada sauce <NEXT_INPUT> green chilies <NEXT_INPUT> cream of mushroom soup <NEXT_INPUT> cream of chicken soup <INPUT_END> <INGR_START> 2 lb. ground beef, browned and drained <NEXT_INGR> tortilla chips <NEXT_INGR> chopped onion <NEXT_INGR> 2 c. Cheddar cheese, grated <NEXT_INGR> 1 can enchilada sauce (hot or mild, to your taste) <NEXT_INGR> 1 can green chilies <NEXT_INGR> 1 can cream of mushroom soup <NEXT_INGR> 1 can cream of chicken soup <INGR_END> <INSTR_START> Mix the enchilada sauce, chilies and soups together. <NEXT_INSTR> Layer as follows in casserole dish: chips; half of beef; layer of onions, about 1/2 cup; 1 cup cheese; half of soup mixture. <NEXT_INSTR> Repeat layers, top with soup mixture. <NEXT_INSTR> Bake at 350° about 45 minutes. <INSTR_END> <TITLE_START> Enchilada Casserole <TITLE_END> <RECIPE_END> <RECIPE_START> <INPUT_START> beef stock <NEXT_INPUT> Cajun Seasoning <NEXT_INPUT> cooked meat <NEXT_INPUT> soy sauce <NEXT_INPUT> eggs <NEXT_INPUT> green onions <INPUT_END> <INGR_START> 1 (8-ounce) package spaghetti <NEXT_INGR> 2 quarts beef stock <NEXT_INGR> 1 teaspoon Cajun Seasoning <NEXT_INGR> 2 cups cooked meat (beef, chicken, pork, shrimp, turtle), finely chopped <NEXT_INGR> 2 tablespoons soy sauce <NEXT_INGR> 3 hard-boiled eggs, peeled and halved lengthwise <NEXT_INGR> 1 bunch green onions, finely chopped <INGR_END> <INSTR_START> Cook spaghetti according to package directions. While spaghetti is cooking, bring stock and Cajun Seasoning to a boil in a large pot. Add meat and simmer 5 minutes. Reserving stock, strain meat out. Set meat aside and keep warm. Bring stock back to a boil and stir in soy sauce. Simmer 1 minute. <NEXT_INSTR> To assemble, place drained spaghetti in 6 individual bowls. Divide meat and egg halves over spaghetti. Ladle on broth and sprinkle with green onions. <INSTR_END> <TITLE_START> Yaka Mein <TITLE_END> <RECIPE_END> <RECIPE_START> <INPUT_START> flour <NEXT_INPUT> salt <NEXT_INPUT> vegetable shortening <NEXT_INPUT> water <NEXT_INPUT> Cooking spray <NEXT_INPUT> brown sugar <NEXT_INPUT> vegetable shortening <NEXT_INPUT> boiling water <NEXT_INPUT> baking soda <NEXT_INPUT> molasses <INPUT_END> <INGR_START> 2 cups all-purpose flour, divided <NEXT_INGR> 1/2 teaspoon salt, divided <NEXT_INGR> 1/4 cup vegetable shortening <NEXT_INGR> 3 1/2 tablespoons ice water <NEXT_INGR> Cooking spray <NEXT_INGR> 1/2 cup packed brown sugar <NEXT_INGR> 3 tablespoons vegetable shortening <NEXT_INGR> 1 cup boiling water <NEXT_INGR> 1 teaspoon baking soda <NEXT_INGR> 1 cup molasses <INGR_END> <INSTR_START> Lightly spoon flour into dry measuring cups; level with a knife. Combine 1 cup flour and 1/4 teaspoon salt in a bowl; cut in 1/4 cup shortening with a pastry blender or 2 knives until mixture resembles coarse meal. Sprinkle surface with ice water, 1 tablespoon at a time; toss with a fork until moist and crumbly (do not form a ball). <NEXT_INSTR> Press mixture gently into a 4-inch circle on heavy-duty plastic wrap; cover with additional plastic wrap. Roll dough, still covered, to a 12-inch circle. Freeze 10 minutes or until plastic wrap can be easily removed. <NEXT_INSTR> Remove 1 sheet of plastic wrap; fit dough into a 9-inch pie plate coated with cooking spray. Remove top sheet of plastic wrap. Fold edges under; flute. <NEXT_INSTR> Preheat oven to 350°. <NEXT_INSTR> Combine 1 cup flour, 1/4 teaspoon salt, and brown sugar in a bowl; cut in 3 tablespoons shortening with a pastry blender or 2 knives until mixture resembles coarse meal. Combine boiling water and baking soda; stir in molasses. Pour molasses mixture into prepared crust; sprinkle flour mixture over molasses mixture. Place pie on a baking sheet; bake at 350° for 40 minutes or until set. Cool on a wire rack. <INSTR_END> <TITLE_START> Shoofly Pie <TITLE_END> <RECIPE_END> <RECIPE_START> <INPUT_START> olive oil <NEXT_INPUT> onion <NEXT_INPUT> zucchini <NEXT_INPUT> salt <NEXT_INPUT> corn <NEXT_INPUT> eggs <NEXT_INPUT> milk <NEXT_INPUT> white cheese <INPUT_END> <INGR_START> 1 tablespoon olive oil <NEXT_INGR> 1 small onion, diced <NEXT_INGR> 3 large zucchini or 6 small zucchini <NEXT_INGR> salt and pepper <NEXT_INGR> 1 12 cups frozen corn <NEXT_INGR> 3 large eggs <NEXT_INGR> 34 cup milk <NEXT_INGR> 12-1 cup shredded white cheese (Monterray Jack, etc.) <INGR_END> <INSTR_START> Dice onions and shred zucchini. <NEXT_INSTR> Saute in 1 TB olive oil, adding water if needed, until soft. <NEXT_INSTR> Add frozen corn and stir in. <NEXT_INSTR> Drain. <NEXT_INSTR> Beat eggs, add milk, salt and pepper and cheese. <NEXT_INSTR> Mix egg mixture with vegetables. <NEXT_INSTR> Pour into 9 x 13 pan. <NEXT_INSTR> Bake at 375* until golden brown on edges and the center is solid. <INSTR_END> <TITLE_START> Summer Vegetable Gratin <TITLE_END> <RECIPE_END> <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(line, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e68a6eb-2b91-4952-b3c3-b3d7d86e27ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319961, 2048)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd089864-692f-4b52-8a72-c27452d803d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dataset(s) incompatible with Pandas data types, not table, or no datasets found in HDF5 file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_pandas(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlg/lib/python3.9/site-packages/pandas/io/pytables.py:425\u001b[0m, in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m groups \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39mgroups()\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset(s) incompatible with Pandas data types, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot table, or no datasets found in HDF5 file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m     )\n\u001b[1;32m    429\u001b[0m candidate_only_group \u001b[38;5;241m=\u001b[39m groups[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# For the HDF file to have only one dataset, all other groups\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# should then be metadata groups for that candidate group. (This\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# assumes that the groups() method enumerates parent groups\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# before their children.)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Dataset(s) incompatible with Pandas data types, not table, or no datasets found in HDF5 file."
     ]
    }
   ],
   "source": [
    "dataset = datasets.Dataset.from_pandas(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e768c370-3d88-4ac8-a956-4fe4bda02747",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": byte}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4310cf92-f71b-4613-97d5-79fb70dd0ed6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_dataloader\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "926a8cc6-5b0d-47b3-81cb-b3167da26eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dataset(s) incompatible with Pandas data types, not table, or no datasets found in HDF5 file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m sentences \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mDatasetDict(\n\u001b[1;32m      6\u001b[0m     {\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(df_train)\n\u001b[1;32m      8\u001b[0m     }\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlg/lib/python3.9/site-packages/pandas/io/pytables.py:425\u001b[0m, in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m groups \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39mgroups()\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset(s) incompatible with Pandas data types, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot table, or no datasets found in HDF5 file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m     )\n\u001b[1;32m    429\u001b[0m candidate_only_group \u001b[38;5;241m=\u001b[39m groups[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# For the HDF file to have only one dataset, all other groups\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# should then be metadata groups for that candidate group. (This\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# assumes that the groups() method enumerates parent groups\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# before their children.)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Dataset(s) incompatible with Pandas data types, not table, or no datasets found in HDF5 file."
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "df_train = pd.read_hdf(path)\n",
    "sentences = datasets.DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_pandas(df_train)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498e7bde-070c-4b5a-9120-1a5890d0d60f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlg (3.9.16)",
   "language": "python",
   "name": "nlg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
