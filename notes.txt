25-07-2023
DID:
- Fixed a bug in the method get_ingredients() from prepare_evaluation.py (where the number of spaces between the generated ingredient list was not consistent and penalized the generation.
-

TRIED:
- to calculate the pourcentage of ingredients seen in the instructions -> failed in some cases and gave 0% since one recipe has instruction: "Mix the first 6 ingredients."
- tried to remove the recipe every recipe where there are zero matches. Not working so well other "Mix first 4 ingredients" where the result is not 0% overlap.
- Conclusion: we need to decide what we want for our metric
	1. we leave everything as is (gets 0.50 or 50% coverage of ingredients with about 62 out of 1000 0%) - this tells us about how many times the model uses terms like "combine all/some..." or "mix all/some"
	2. we try to remove or take into account recipes using "combine all/some..." or "mix all/some" 

TODO:
- Finish ingredient quantity counting using NER for reverse way
- Add test to account for duplicate ingredients in input (generated by the model most probably as an error)

(later)
- do the evaluation for the quantities of ingredients used/present
- try to check if the padding (and other settings of the tokenizers) between our models (GPT2 and OPT) is similar to what Bien et al. did so that their generation algorithm works best before training new models.
